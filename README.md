# ğŸ“˜ Digit Classification using CNN on MNIST

A complete deepâ€‘learning pipeline using TensorFlow/Keras

This repository contains a full implementation of a Convolutional Neural Network (CNN) for handwritten digit classification using the MNIST dataset.

The project follows the official Keras example and expands it with preprocessing, visualization, model evaluation, confusion matrix, and training diagnostics.


## âœ… Project Overview

The goal of this project is to build a CNN that can classify handwritten digits (0â€“9) from the MNIST dataset.

The workflow includes:

- Setting reproducible seeds
  
- Loading and visualizing MNIST data
  
- Preprocessing (normalization, reshaping, oneâ€‘hot encoding)
  
- Designing a CNN with Conv2D â†’ BatchNorm â†’ ReLU â†’ MaxPooling blocks
  
- Training the model with validation
  
- Evaluating performance on test data
  
- Generating predictions
  
- Plotting accuracy/loss curves
  
- Creating a confusion matrix
  
- Generating a classification report
  

## ğŸ“‚ Dataset

### MNIST consists of:

- 60,000 training images
  
- 10,000 test images
  
- Grayscale images of size 28 Ã— 28 Ã— 1
  
- Labels from 0 to 9
  

## ğŸ§ª Environment Setup

`conda install tensorflow`

`pip install numpy pandas matplotlib seaborn scikit-learn`



## ğŸ§± Model Architecture

The CNN follows this structure:

`Input (28Ã—28Ã—1)                                 `

`â”‚                                               `

`â”œâ”€â”€ Conv2D (8 filters, 3Ã—3, same padding)       `

`â”œâ”€â”€ BatchNormalization                          `

`â”œâ”€â”€ ReLU                                        `

`â”œâ”€â”€ MaxPooling2D (2Ã—2)                          `

`â”‚                                               `

`â”œâ”€â”€ Conv2D (16 filters, 3Ã—3, same padding)      `

`â”œâ”€â”€ BatchNormalization                          `

`â”œâ”€â”€ ReLU                                        `

`â”œâ”€â”€ MaxPooling2D (2Ã—2)                          `

`â”‚                                               `

`â”œâ”€â”€ Conv2D (32 filters, 3Ã—3, same padding)      `

`â”œâ”€â”€ BatchNormalization                          `

`â”œâ”€â”€ ReLU                                        `

`â”‚                                               `

`â”œâ”€â”€ Flatten                                     `

`â”œâ”€â”€ Dense (128 units, ReLU)                     `

`â””â”€â”€ Dense (10 units, Softmax)                   `


This design extracts hierarchical features and reduces spatial dimensions while increasing depth.

## ğŸ› ï¸ Code Workflow

1. Set Random Seeds

Ensures reproducibility for NumPy and TensorFlow.

2. Load MNIST Dataset

`(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()`


3. Visualize Sample Images
   
Displays the first 9 training images.

4. Preprocessing
 
- Normalize pixel values to [0,1]
  
- Reshape to (28,28,1)
  
- Oneâ€‘hot encode labels using to_categorical
  
5. Build the CNN Model
  
Uses `Sequential()` with Conv2D, BatchNorm, ReLU, MaxPooling, Dense, and Softmax layers.

6. Compile the Model

`model.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])`


7. Train the Model
   
Uses 20% of training data for validation.

8. Evaluate on Test Data
    
Outputs test accuracy and loss.

9. Predictions
    
Predicts the first 5 test images and compares with ground truth.

10. Training Curves
    
Plots accuracy and loss for both training and validation.

11. Confusion Matrix

Visualizes model performance across all classes.

12. Classification Report
    
Shows precision, recall, and F1â€‘score for each digit.


## ğŸ“Š Results 

### Typical performance:

- Training accuracy: ~99â€“100%
  
- Validation accuracy: ~98%
  
- Test accuracy: ~98%
  
- Confusion matrix shows strong performance across all digits.
  

## ğŸ“ˆ Visualizations Included

- Sample MNIST images
  
- Training vs validation accuracy
  
- Training vs validation loss
  
- Confusion matrix heatmap
  
- Classification report
  

## ğŸ“¦ Files in This Repository

`â”œâ”€â”€ mnist_cnn.ipynb / mnist_cnn.py   # Main code                          `

`â”œâ”€â”€ model_plot.png                   # Model architecture visualization   `

`â”œâ”€â”€ README.md                        # Project documentation              `



## ğŸš€ Future Improvements

- Add dropout layers to reduce overfitting
  
- Experiment with Adam optimizer
  
- Add data augmentation
  
- Try deeper architectures (LeNetâ€‘5, VGGâ€‘style)

## Kaggle link for reference:

**https://www.kaggle.com/code/mohitrajemdramahajan/digit-classification-using-mnist**
